{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-5d55143d77bd>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-5d55143d77bd>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    df$Classification=factor(df$Classification)\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns # data visualization library  \n",
    "import matplotlib.pyplot as plt\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import time\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "\n",
    "df=pd.read_csv(\"C:\\Users\\mansour\\Desktop\\python\\Misk_Learn_Py\\data\\dataR2.csv\")\n",
    "df\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(transformed_X,y,test_size=0.3)\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "linear=LinearRegression()\n",
    "linear.fit(X_train,y_train)\n",
    "\n",
    "linear.score(X_test,y_test)\n",
    "\n",
    "\n",
    "#If MCP count is between 0-250 then there is a chance to get cancer\n",
    "\n",
    "#Finding correlation\n",
    "#when we have various variables,Correlation is an important factor to check the dependencies within themselves\n",
    "#its gives us an insight between mutual relationship among variables\n",
    "#to get correlation among  different variables for a data set use following code\n",
    "\n",
    "\n",
    "plot(df)\n",
    "#below codes gives graphical representation of correlation\n",
    "\n",
    "cr=cor(df)\n",
    "\n",
    "df.cor = cor(df)\n",
    "corrplot(df.cor)\n",
    "#corrplot(cr,type='lower')\n",
    "corrplot(cr,method='number')\n",
    "#from the above code we are getting HOMA AND Insulin are multicollinear varilables\n",
    "\n",
    "#splitting the data into train and test\n",
    "\n",
    "set.seed(101)\n",
    "sample=sample.split(df$MCP.1,SplitRatio = .7)\n",
    "train=subset(df,sample==T)\n",
    "test=subset(df,sample==F)\n",
    "\n",
    "#Multicollinearity\n",
    "#Multicollinearity makes it hard to interpret your coefficients,\n",
    "#and it reduces the power of your model to identify independent variables that are statistically significant.\n",
    "#These are definitely serious problems\n",
    "\n",
    "\n",
    "df=subset(df,select = -c(MCP.1))\n",
    "numericData=df[sapply(df,is.numeric)]\n",
    "descrCor=cor(numericData)\n",
    "\n",
    "#vif\n",
    "\n",
    "model=lm(MCP.1~.,data=train)\n",
    "vif(model)\n",
    "#from the above code we will get multicollinearity varibles Insulin and Homa,we need remove those coulmns\n",
    "\n",
    "#Now create the model\n",
    "model=lm(MCP.1~.,data=train)\n",
    "summary(model)\n",
    "\n",
    "#from summary  we came to know there is less correaltion bw all columns related to target column\n",
    "\n",
    "#model creation after removing Insulin and Adiponectin\n",
    "model=lm(MCP.1~BMI+Resistin+Glucose+HOMA+Insulin+Leptin,data=train)\n",
    "summary(model)\n",
    "\n",
    "pred=predict(model,test)\n",
    "pred\n",
    "\n",
    "#Comparing Predicted vs Actual\n",
    "plot(test$MCP.1,type = \"l\",lyt=1.8,col=\"red\")\n",
    "lines(pred,type=\"l\",col=\"blue\")\n",
    "\n",
    "plot(pred,type='l',lyt=1.8,col='blue')#from this data is not seems to be good\n",
    "\n",
    "#Finding Accuracy\n",
    "accuracy=sqrt(mean(pred-df$MCP.1)^2)\n",
    "\n",
    "#converting categorical into binary features\n",
    "df$Classification=factor(df$Classification,labels=c(0,1))\n",
    "\n",
    "#Model split\n",
    "set.seed(101)\n",
    "sample=sample.split(df$Classification,SplitRatio = 0.7)\n",
    "train=subset(df,sample==TRUE)\n",
    "test=subset(df,sample==FALSE)\n",
    "#model building\n",
    "model=glm(Classification~.,data=train,family='binomial')\n",
    "summary(model)\n",
    "\n",
    "#we need remove this one because its has less significance levels\n",
    "Adiponectin\n",
    "\n",
    "#predictions\n",
    "res=predict(model,test,type='response')\n",
    "res\n",
    "#confusion matrix\n",
    "table(ActualValue=test$Classification ,PredictedValue=res>0.5)\n",
    "\n",
    "#Decision Tree\n",
    "\n",
    "#Model Building\n",
    "tree=rpart(Classification~.,data=train)\n",
    "tree\n",
    "\n",
    "#we can plot\n",
    "plot(tree,margin=0.1)\n",
    "\n",
    "#margin is used to adjust the plot ,for viewing labels\n",
    "text(tree,use.n = TRUE,pretty = TRUE,cex=0.8)\n",
    "\n",
    "#Predictions\n",
    "pred=predict(tree,test,type='class')\n",
    "pred\n",
    "\n",
    "#Confusion matrix for evaluating the model\n",
    "confusionMatrix(pred,test$Classification)\n",
    "#Sensitivity says actual positive  cases\n",
    "#specialty says negative values\n",
    "\n",
    "\n",
    "#Random Forest,  works in regression and classification\n",
    "#before Build a model we need to check the significance level of each variable for that we will use tunaRE function\n",
    "\n",
    "bestmtry=tuneRF(train,train$Classification,stepFactor = 1.2,improve = 0.01,trace=T,plot=T)\n",
    "\n",
    "#OOB nothing but prediction error\n",
    "#so our actual consider columns are max 3 enough for build a model\n",
    "\n",
    "\n",
    "model=randomForest(Classification~.,data=train)\n",
    "model\n",
    "\n",
    "#No. of variables tried at each split: 3\n",
    "#Every node will split into 3 daughter nodes\n",
    "#  OOB estimate of  error rate: 32.1% its depends upon confusion matrix\n",
    "\n",
    "#we need find the gini information for tree splitting\n",
    "\n",
    "importance(model)\n",
    "\n",
    "#visualizing the gini index\n",
    "varImpPlot(model)\n",
    "\n",
    "#predicting the model\n",
    "pred=predict(model,test,type = 'class')\n",
    "pred\n",
    "\n",
    "\n",
    "#Confusion Matrix\n",
    "confusionMatrix(table(pred,test$Classification))\n",
    "\n",
    "\n",
    "\n",
    "#KNN\n",
    "#Using Of KNN data Should be Labeled (structured format)\n",
    "#Data Should be Noise free\n",
    "#data should be small because KNN is lazy learner\n",
    "#According to Ecludian distance we can calculate distance then we can say\n",
    "#how many observations are similar based on their distance\n",
    "\n",
    "#K-Nearest Neighbor also known as KNN is a supervised learning algorithm that can be used for regression as well as classification problems.\n",
    "#But KNN is widely used for classification problems in machine learning.\n",
    "#KNN works on a principle assuming every data point falling near to each other is falling in the same class.\n",
    "#That means similar things are near to each other.\n",
    "\n",
    "#KNN algorithms decide a number k which is the nearest Neighbor to that data point which is to be classified.\n",
    "#If the value of k is 5 it will look for 5 nearest Neighbors to that data point.\n",
    "#lets the example, k=4. KNN finds out 4 nearest Neighbors.\n",
    "#It is seen that because that data point is close to these Neighbors so it will belong to this class only.\n",
    "\n",
    "#What is the significance of k?\n",
    "\n",
    "#Value of k - bigger the value of k increases confidence in the prediction.\n",
    "\n",
    "#Load the desired data.\n",
    "\n",
    "#Choose the value of k.\n",
    "\n",
    "#For getting the class which is to be predicted, repeat starting from 1 to the total number of training points we have.\n",
    "\n",
    "#The next step is to calculate the distance between the data point whose class is to be predicted and all the training data points. Euclidean distance can be used here.\n",
    "\n",
    "#Arrange the distances in non-decreasing order.\n",
    "\n",
    "#Assume the positive value of k and filtering k lowest values from the sorted list.\n",
    "\n",
    "#We have top k top distances.\n",
    "\n",
    "\n",
    "#Normalize the data because by doing this we can get all observations similar format\n",
    "normalize=function(x){\n",
    " return((x-min(x)) / (max(x)-min(x)))}\n",
    "nor=as.data.frame(lapply(df[,1:9],normalize))#By running this will get all normalized values\n",
    "head(nor)\n",
    "\n",
    "#Model splitting\n",
    "\n",
    "set.seed(101)\n",
    "\n",
    "build=sample(10:nrow(nor),size=nrow(nor)*0.7 ,replace=FALSE)\n",
    "\n",
    "train.class=df[build,]#70% training data\n",
    "test.class=df[-build,]# remaining 30% test data\n",
    "\n",
    "#Now creating a separate data frame for 'Classification' feature which is our target.\n",
    "train.class_labels=df[build,10]\n",
    "test.class_labels=df[-build,10]\n",
    "\n",
    "\n",
    "\n",
    "NROW(train.class_labels)# To find number of observations in our training data in target\n",
    "\n",
    "\n",
    "#Optimization of K value\n",
    "\n",
    "i=1\n",
    "k.optm=1\n",
    "for (i in 1:28) {\n",
    " knn.mod=knn(train=train.class ,test=test.class,cl=train.class_labels ,k=i)\n",
    " k.optm[i]=100*sum(test.class_labels==knn.mod)/NROW(test.class_labels)\n",
    " k=i\n",
    " cat(k,'=',k.optm[i],'\\n')  # to print accuracy\n",
    "}\n",
    "\n",
    "plot(k.optm, type='b',xlab = 'k-value' ,ylab='Accuracy level') # To plot % accuracy wrt to k=value\n",
    "\n",
    "\n",
    "knn.2=knn(train=train.class,test.class,cl=train.class_labels,k=2)\n",
    "ACC.2=100*sum(test.class_labels==knn.2)/NROW(test.class_labels)  #k=2\n",
    "ACC.2\n",
    "\n",
    "# To check predictions against actual value in tabular form\n",
    "table(knn.2,test.class_labels)\n",
    "\n",
    "\n",
    "confusionMatrix(table(knn.2,test.class_labels))\n",
    "\n",
    "\n",
    "#MARS\n",
    "# Many of these models can be adapted to nonlinear patterns in the data by manually adding model terms (i.e. squared terms, interaction effects);\n",
    "#however, to do so you must know the specific nature of the nonlinearity a priori.\n",
    "#Alternatively, there are numerous algorithms that are inherently nonlinear.\n",
    "#When using these models, the exact form of the nonlinearity does not need to be known explicitly or specified prior to model training.\n",
    "#Rather, these algorithms will search for, and discover, nonlinearities in the data that help maximize predictive accuracy.\n",
    "\n",
    "\n",
    "\n",
    "# Fit a basic MARS model\n",
    "mars1=earth(MCP.1~ .,data =df)\n",
    "mars1\n",
    "# Print model summary\n",
    "print(mars1)\n",
    "\n",
    "#It also shows us that 6 of 16 terms were used from 5 of the 7 original predictors.\n",
    "#But what does this mean? If we were to look at all the coefficients,\n",
    "#we would see that there are 6 terms in our model (including the intercept).\n",
    "#These terms include hinge functions produced\n",
    "#from the original 7 predictors (7 predictors because the model automatically dummy encodes our categorical variables).\n",
    "\n",
    "summary(mars1) %>% .$coefficients %>% head(10)\n",
    "\n",
    "#The plot method for MARS model objects provide convenient performance and residual plots.\n",
    "\n",
    "plot(mars1, which = 1)\n",
    "\n",
    "# the model selection plot that graphs the\n",
    "#GCV R2( generalized cross validation criterion. GCV is a form of regularization: it trades off goodness-of-fit against model complexity.)\n",
    "#(left-hand y-axis and solid black line) based on the number of terms retained in the model\n",
    "#(x-axis) which are constructed from a certain number of original predictors (right-hand y-axis).\n",
    "#The vertical dashed lined at 37 tells us the optimal number of non-intercept terms retained where marginal increases in\n",
    "\n",
    "#Tuning\n",
    "#Since there are two tuning parameters associated with our MARS model:\n",
    "#the degree of interactions and the number of retained terms,\n",
    "#we need to perform a grid search to identify the optimal combination of these hyperparameters that minimize prediction error\n",
    "\n",
    "# create a tuning grid\n",
    "hyper_grid=expand.grid(\n",
    " degree = 1:3,\n",
    " nprune = seq(2, 100, length.out = 10) %>% floor()\n",
    ")\n",
    "\n",
    "head(hyper_grid)\n",
    "\n",
    "#We can use caret to perform a grid search using 10-fold cross-validation\n",
    "# for reproducibiity\n",
    "set.seed(123)\n",
    "\n",
    "# cross validated model\n",
    "tuned_mars <- train(\n",
    " x = subset(df, select = -MCP.1),\n",
    " y = df$MCP.1,\n",
    " method = \"earth\",\n",
    " metric = \"RMSE\",\n",
    " trControl = trainControl(method = \"cv\", number = 10),\n",
    " tuneGrid = hyper_grid\n",
    ")\n",
    "\n",
    "# best model\n",
    "tuned_mars$bestTune\n",
    "##    nprune degree\n",
    "## 14     34      2\n",
    "\n",
    "# plot results\n",
    "ggplot(tuned_mars)\n",
    "\n",
    "#The following table compares the cross-validated RMSE for our tuned MARS model\n",
    "#to a regular multiple regression model along with tuned principal component regression (PCR),\n",
    "#partial least squares (PLS), and regularized regression (elastic net) models.\n",
    "#By incorporating non-linear relationships and interaction effects,\n",
    "#the MARS model provides a substantial improvement over the previous linear models that we have explored.\n",
    "\n",
    "#Notice\n",
    "#Notice that we standardize the features for the linear models but we did not for the MARS model.\n",
    "#Whereas linear models tend to be sensitive to the scale of the features,\n",
    "#MARS models are not.\n",
    "\n",
    "# multiple regression\n",
    "set.seed(123)\n",
    "cv_model1 <- train(\n",
    " MCP.1 ~ .,\n",
    " data = df,\n",
    " method = \"lm\",\n",
    " metric = \"RMSE\",\n",
    " trControl = trainControl(method = \"cv\", number = 10),\n",
    " preProcess = c(\"zv\", \"center\", \"scale\")\n",
    ")\n",
    "\n",
    "# principal component regression\n",
    "set.seed(123)\n",
    "cv_model2 <- train(\n",
    " MCP.1 ~ .,\n",
    " data = df,\n",
    " method = \"pcr\",\n",
    " trControl = trainControl(method = \"cv\", number = 10),\n",
    " metric = \"RMSE\",\n",
    " preProcess = c(\"zv\", \"center\", \"scale\"),\n",
    " tuneLength = 20\n",
    ")\n",
    "\n",
    "# partial least squares regression\n",
    "set.seed(123)\n",
    "cv_model3 <- train(\n",
    " MCP.1 ~ .,\n",
    " data =df,\n",
    " method = \"pls\",\n",
    " trControl = trainControl(method = \"cv\", number = 10),\n",
    " metric = \"RMSE\",\n",
    " preProcess = c(\"zv\", \"center\", \"scale\"),\n",
    " tuneLength = 20\n",
    ")\n",
    "\n",
    "# regularized regression\n",
    "set.seed(123)\n",
    "cv_model4 <- train(\n",
    " MCP.1 ~ .,\n",
    " data =df,\n",
    " method = \"glmnet\",\n",
    " trControl = trainControl(method = \"cv\", number = 10),\n",
    " metric = \"RMSE\",\n",
    " preProcess = c(\"zv\", \"center\", \"scale\"),\n",
    " tuneLength = 10\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# extract out of sample performance measures\n",
    "summary(resamples(list(\n",
    " Multiple_regression = cv_model1,\n",
    " PCR = cv_model2,\n",
    " PLS = cv_model3,\n",
    " Elastic_net = cv_model4,\n",
    " MARS = tuned_mars\n",
    ")))$statistics$RMSE %>%\n",
    " kableExtra::kable() %>%\n",
    " kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n",
    "\n",
    "#Feature interpretation\n",
    "#MARS models via earth::earth() include a backwards elimination feature selection routine\n",
    "#that looks at reductions in the GCV estimate of error as each predictor is added to the model.\n",
    "#This total reduction is used as the variable importance measure (value = \"gcv\").\n",
    "#Since MARS will automatically include and exclude terms during the pruning process,\n",
    "#it essentially performs automated feature selection.\n",
    "#If a predictor was never used in any of the MARS basis functions in the final model (after pruning),\n",
    "#it has an importance value of zero.\n",
    "\n",
    "#while the rest of the features have an importance value of zero since they were not included in the final model.\n",
    "#Alternatively, you can also monitor the change in the residual sums of squares (RSS) as terms are added (value = \"rss\");\n",
    "#however, you will see very little difference between these methods.\n",
    "\n",
    "# variable importance plots\n",
    "p1 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = \"gcv\") + ggtitle(\"GCV\")\n",
    "p2 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = \"rss\") + ggtitle(\"RSS\")\n",
    "\n",
    "gridExtra::grid.arrange(p1, p2, ncol = 2)\n",
    "\n",
    "#Its important to realize that variable importance will only measure the impact of the prediction error\n",
    "#as features are included; however, it does not measure the impact for particular hinge functions created for a given feature.\n",
    "#For example, in Figure  we see that Resistin and Glucose are the two most influential variables;\n",
    "#however, variable importance does not tell us how our model is treating the non-linear patterns for each feature.\n",
    "#Also, if we look at the interaction terms our model retained,\n",
    "#we see interactions between different hinge functions for  Resistin and Glucose.\n",
    "\n",
    "\n",
    "#MARS provides a great stepping stone into nonlinear modeling and tends to be fairly intuitive due to being closely related to multiple regression techniques.\n",
    "#They are also easy to train and tune.\n",
    "#From this illustrated how incorporating non-linear relationships via MARS modeling greatly\n",
    "#improved predictive accuracy on our Brest Cancer data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-49981303b4a8>, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-49981303b4a8>\"\u001b[1;36m, line \u001b[1;32m42\u001b[0m\n\u001b[1;33m    df$Classification=factor(df$Classification)\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Using Of KNN data Should be Labeld (structed format)\n",
    "#Data Should be Noise free\n",
    "#data should be small bcz KNN is lazy learner \n",
    "#According to Ecludian distance we can calculate distance then we can say \n",
    "#how many oobservations are simmilrty based on their ddistance\n",
    "\n",
    "#K-Nearest Neighbor also known as KNN is a supervised learning algorithm that can be used for regression as well as classification problems.\n",
    "#But KNN is widely used for classification problems in machine learning. \n",
    "#KNN works on a principle assuming every data point falling near to each other is falling in the same class.\n",
    "#That means similar things are near to each other.\n",
    "\n",
    "#KNN algorithms decide a number k which is the nearest Neighbor to that data point which is to be classified.\n",
    "#If the value of k is 5 it will look for 5 nearest Neighbors to that data point. \n",
    "#lets the example, k=4. KNN finds out 4 nearest Neighbors. \n",
    "#It is seen that because that data point is close to these Neighbors so it will belong to this class only. \n",
    "\n",
    "#What is the significance of k?\n",
    "\n",
    "#Value of k â€“ bigger the value of k increases confidence in the prediction. \n",
    "\n",
    "#Load the desired data. \n",
    "\n",
    "#Choose the value of k.\n",
    "\n",
    "#For getting the class which is to be predicted, repeat starting from 1 to the total number of training points we have.\n",
    "\n",
    "#The next step is to calculate the distance between the data point whose class is to be predicted and all the training data points. Euclidean distance can be used here.\n",
    "\n",
    "#Arrange the distances in non-decreasing order. \n",
    "\n",
    "#Assume the positive value of k and filtering k lowest values from the sorted list.\n",
    "\n",
    "#We have top k top distances.\n",
    "\n",
    "df=read.csv(\"C:\\\\Users\\\\VENKY\\\\Downloads\\\\dataR2.csv\")\n",
    "nrow(df)\n",
    "ncol(df)\n",
    "head(df)\n",
    "tail(df)\n",
    "str(df)\n",
    "#Conversion\n",
    "df$Classification=factor(df$Classification)\n",
    "str(df)\n",
    "summary(df)\n",
    "#finding null values\n",
    "which(is.na(df))\n",
    "any(is.na(df))\n",
    "df[1,4]\n",
    "##Visualization\n",
    "library(ggplot2)\n",
    "# plain gragh is seen\n",
    "ggplot(data=df, aes(x=BMI, y=MCP.1))\n",
    "#add  geometry  \n",
    "ggplot(data=df, aes(x=BMI, y=MCP.1))+ geom_point()\n",
    "#add  color\n",
    "#Lines\n",
    "a=ggplot(data=df, aes(x=BMI, y=MCP.1, colour=Classification))+geom_point()\n",
    "a\n",
    "#add  size\n",
    "ggplot(data=df, aes(x=Insulin, y=MCP.1, colour=Classification, Size=BMI))+ geom_point()\n",
    "#-----ploting with layers\n",
    "p<-ggplot(data=df, aes(x=Insulin, y=MCP.1, colour=Classification))\n",
    "# point\n",
    "a+geom_point(aes(size=BMI))\n",
    "#multiple layer\n",
    "p+geom_point()+geom_line()\n",
    "##If the insulin is in the range of 0-20 , \n",
    "#MCP-1 is ranging in between 230 -1050 then there is a chance that the person can get brest cancer \n",
    "\n",
    "#------------ mapping vs settign \n",
    "r<-ggplot(data=df, aes(x=Glucose, y=MCP.1))\n",
    "# add color\n",
    "# mapping \n",
    "r+geom_point(aes(colour=Classification))\n",
    "# setting\n",
    "# mapping \n",
    "r+geom_point(aes(size=Classification))\n",
    "#If your glucose and MCP level are high then there is a change that you may get cancer\n",
    "# setting\n",
    "r+geom_point(size=10)\n",
    "#----Denisty chart\n",
    "s+geom_density(aes(fill=Classification))\n",
    "\n",
    "#converting 1 and 2 into 0 and 1\n",
    "df$Classification=factor(df$Classification,labels=c(0,1))\n",
    "\n",
    "#Normalize the data bcz by doing this we can get all observtions similar format\n",
    "normalize=function(x){\n",
    "  return((x-min(x)) / (max(x)-min(x)))}\n",
    "nor=as.data.frame(lapply(df[,1:9],normalize))#By running this will get all normalized values\n",
    "head(nor)\n",
    "\n",
    "#Model splitting\n",
    "library(caTools)\n",
    "set.seed(101)\n",
    "\n",
    "build=sample(10:nrow(nor),size=nrow(nor)*0.7 ,replace=FALSE)\n",
    "\n",
    "train.class=df[build,]#70% traing data\n",
    "test.class=df[-build,]# remiang 30% testdata\n",
    "\n",
    "#Now creating separate dataframe for 'Classification' feature which is our traget.\n",
    "train.class_labels=df[build,10]\n",
    "test.class_labels=df[-build,10]\n",
    "\n",
    "install.packages('class')\n",
    "library(class)\n",
    "\n",
    "NROW(train.class_labels)# To find number of observations in our training data in target\n",
    " \n",
    "\n",
    "#Optimization of K value\n",
    "\n",
    "i=1\n",
    "k.optm=1\n",
    "for (i in 1:28) {\n",
    "  knn.mod=knn(train=train.class ,test=test.class,cl=train.class_labels ,k=i)\n",
    "  k.optm[i]=100*sum(test.class_labels==knn.mod)/NROW(test.class_labels)\n",
    "  k=i\n",
    "  cat(k,'=',k.optm[i],'\\n')  # to print accuracy\n",
    "}\n",
    "\n",
    "plot(k.optm, type='b',xlab = 'k-value' ,ylab='Accuracy level') # To plot % accuracy wrt to k=value\n",
    "\n",
    "\n",
    "knn.5=knn(train=train.class,test.class,cl=train.class_labels,k=5)\n",
    "ACC.5=100*sum(test.class_labels==knn.5)/NROW(test.class_labels)  #k=5\n",
    "ACC.5\n",
    "\n",
    "# To check predictions against actual value in tabular form \n",
    "table(knn.5,test.class_labels)\n",
    "\n",
    "library(caret)\n",
    "confusionMatrix(table(knn.5,test.class_labels))\n",
    "\n",
    "\n",
    "# before ANd traditional\n",
    "knn.24=knn(train=train.class,test.class,cl=train.class_labels,k=24)\n",
    "knn.26=knn(train=train.class,test.class,cl=train.class_labels,k=26)\n",
    "\n",
    "#Lets calcuate the proportion of correct classification for k= 24,26\n",
    "\n",
    "ACC.24=100*sum(test.class_labels==knn.24)/NROW(test.class_labels)  #k=24\n",
    "ACC.24\n",
    "ACC.26=100*sum(test.class_labels==knn.26)/NROW(test.class_labels)  #k=26\n",
    "ACC.26\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many of these models can be adapted to nonlinear patterns in the data by manually adding model terms (i.e. squared terms, interaction effects);\n",
    "#however, to do so you must know the specific nature of the nonlinearity a priori.\n",
    "#Alternatively, there are numerous algorithms that are inherently nonlinear. \n",
    "#When using these models, the exact form of the nonlinearity does not need to be known explicitly or specified prior to model training.\n",
    "#Rather, these algorithms will search for, and discover, nonlinearities in the data that help maximize predictive accuracy.\n",
    "\n",
    "install.packages('rsample')\n",
    "install.packages('earth')\n",
    "install.packages('vip')\n",
    "install.packages('pdp')\n",
    "\n",
    "library(rsample)   # data splitting \n",
    "library(ggplot2)   # plotting\n",
    "library(earth)     # fit MARS models\n",
    "library(caret)     # automating the tuning process\n",
    "library(vip)       # variable importance\n",
    "library(pdp)       # variable relationships\n",
    "\n",
    "df=read.csv(\"C:\\\\Users\\\\VENKY\\\\Downloads\\\\dataR2.csv\")\n",
    "nrow(df)\n",
    "ncol(df)\n",
    "head(df)\n",
    "tail(df)\n",
    "str(df)\n",
    "\n",
    "#Conversion\n",
    "df$Classification=factor(df$Classification)\n",
    "df$Age=factor(df$Age)\n",
    "df=subset(df,select = -c(1,10))\n",
    "\n",
    "str(df)\n",
    "summary(df)\n",
    "\n",
    "#finding null values\n",
    "\n",
    "which(is.na(df))\n",
    "any(is.na(df))\n",
    "\n",
    "#Finding Outliers\n",
    "library(ggstatsplot)\n",
    "library(ggplot2)\n",
    "\n",
    "# Create a boxplot of the dataset.\n",
    "boxplot(df,horizontal = T)\n",
    "colnames(df)\n",
    "\n",
    "#for individual column\n",
    "boxplot(df$BMI)#no Outliers\n",
    "boxplot(df$Glucose)\n",
    "boxplot(df$Insulin)\n",
    "boxplot(df$HOMA)\n",
    "boxplot(df$Leptin)\n",
    "boxplot(df$Adiponectin)\n",
    "boxplot(df$Resistin)\n",
    "boxplot(df$MCP.1)#no outliers\n",
    "\n",
    "# You can get the actual values of the outliers with this\n",
    "\n",
    "boxplot(df$MCP.1)$out\n",
    "\n",
    "# Now you can assign the outlier values into a vector\n",
    "\n",
    "outliers=boxplot(df$MCP.1, plot=FALSE)$out\n",
    "print(outliers)\n",
    "\n",
    "#Removing the outliers\n",
    "\n",
    "# First you need find in which rows the outliers are\n",
    "\n",
    "df[which(df$MCP.1 %in% outliers),]\n",
    "\n",
    "# Now you can remove the rows containing the outliers, one possible option is:\n",
    "\n",
    "df = df[-which(df$MCP.1 %in% outliers),]\n",
    "\n",
    "# If you check now with boxplot, you will notice that those pesky outliers are gone\n",
    "\n",
    "boxplot(df$MCP.1)\n",
    "\n",
    "##Visualization\n",
    "library(ggplot2)\n",
    "# plain gragh is seen\n",
    "ggplot(data=df, aes(x=BMI, y=MCP.1))\n",
    "#add  geometry  \n",
    "ggplot(data=df, aes(x=BMI, y=MCP.1))+ geom_point()\n",
    "#add  color\n",
    "#Lines\n",
    "a=ggplot(data=df, aes(x=BMI, y=MCP.1, colour=Classification))+geom_point()\n",
    "a\n",
    "#add  size\n",
    "ggplot(data=df, aes(x=Insulin, y=MCP.1, colour=Classification, Size=BMI))+ geom_point()\n",
    "#-----ploting with layers\n",
    "p<-ggplot(data=df, aes(x=Insulin, y=MCP.1, colour=Classification))\n",
    "# point\n",
    "a+geom_point(aes(size=BMI))\n",
    "#multiple layer\n",
    "p+geom_point()+geom_line()\n",
    "##If the insulin is in the range of 0-20 , \n",
    "#MCP-1 is ranging in between 230 -1050 then there is a chance that the person can get brest cancer \n",
    "\n",
    "#------------ mapping vs settign \n",
    "r<-ggplot(data=df, aes(x=Glucose, y=MCP.1))\n",
    "# add color\n",
    "# mapping \n",
    "r+geom_point(aes(colour=Classification))\n",
    "# setting\n",
    "# mapping \n",
    "r+geom_point(aes(size=Classification))\n",
    "#If your glucose and MCP level are high then there is a change that you may get cancer\n",
    "# setting\n",
    "r+geom_point(size=10)\n",
    "\n",
    "#----Denisty chart\n",
    "s+geom_density(aes(fill=Classification))\n",
    "#IF your MCP count is between 0-250 then there is a chance that u may get cancer\n",
    "\n",
    "# Fit a basic MARS model\n",
    "mars1=earth(MCP.1~ .,data =df)\n",
    "mars1\n",
    "# Print model summary\n",
    "print(mars1)\n",
    "\n",
    "#It also shows us that 6 of 16 terms were used from 5 of the 7 original predictors.\n",
    "#But what does this mean? If we were to look at all the coefficients, \n",
    "#we would see that there are 6 terms in our model (including the intercept). \n",
    "#These terms include hinge functions produced \n",
    "#from the original 7 predictors (7 predictors because the model automatically dummy encodes our categorical variables).\n",
    "\n",
    "summary(mars1) %>% .$coefficients %>% head(10)\n",
    "\n",
    "#The plot method for MARS model objects provide convenient performance and residual plots. \n",
    "\n",
    "plot(mars1, which = 1)\n",
    "\n",
    "# the model selection plot that graphs the\n",
    "#GCV R2( generalized cross validation criterion. GCV is a form of regularization: it trades off goodness-of-fit against model complexity.)\n",
    "#(left-hand y-axis and solid black line) based on the number of terms retained in the model\n",
    "#(x-axis) which are constructed from a certain number of original predictors (right-hand y-axis). \n",
    "#The vertical dashed lined at 37 tells us the optimal number of non-intercept terms retained where marginal increases in\n",
    "\n",
    "#Tuning\n",
    "#Since there are two tuning parameters associated with our MARS model: \n",
    "#the degree of interactions and the number of retained terms,\n",
    "#we need to perform a grid search to identify the optimal combination of these hyperparameters that minimize prediction error\n",
    "\n",
    "# create a tuning grid\n",
    "hyper_grid=expand.grid(\n",
    "  degree = 1:3, \n",
    "  nprune = seq(2, 100, length.out = 10) %>% floor()\n",
    ")\n",
    "\n",
    "head(hyper_grid)\n",
    "\n",
    "#We can use caret to perform a grid search using 10-fold cross-validation\n",
    "# for reproducibiity\n",
    "set.seed(123)\n",
    "\n",
    "# cross validated model\n",
    "tuned_mars <- train(\n",
    "  x = subset(df, select = -MCP.1),\n",
    "  y = df$MCP.1,\n",
    "  method = \"earth\",\n",
    "  metric = \"RMSE\",\n",
    "  trControl = trainControl(method = \"cv\", number = 10),\n",
    "  tuneGrid = hyper_grid\n",
    ")\n",
    "\n",
    "# best model\n",
    "tuned_mars$bestTune\n",
    "##    nprune degree\n",
    "## 14     34      2\n",
    "\n",
    "# plot results\n",
    "ggplot(tuned_mars)\n",
    "\n",
    "#The following table compares the cross-validated RMSE for our tuned MARS model\n",
    "#to a regular multiple regression model along with tuned principal component regression (PCR),\n",
    "#partial least squares (PLS), and regularized regression (elastic net) models.\n",
    "#By incorporating non-linear relationships and interaction effects,\n",
    "#the MARS model provides a substantial improvement over the previous linear models that we have explored.\n",
    "\n",
    "#Notice\n",
    "#Notice that we standardize the features for the linear models but we did not for the MARS model.\n",
    "#Whereas linear models tend to be sensitive to the scale of the features,\n",
    "#MARS models are not.\n",
    "\n",
    "# multiple regression\n",
    "set.seed(123)\n",
    "cv_model1 <- train(\n",
    "  MCP.1 ~ ., \n",
    "  data = df, \n",
    "  method = \"lm\",\n",
    "  metric = \"RMSE\",\n",
    "  trControl = trainControl(method = \"cv\", number = 10),\n",
    "  preProcess = c(\"zv\", \"center\", \"scale\")\n",
    ")\n",
    "\n",
    "# principal component regression\n",
    "set.seed(123)\n",
    "cv_model2 <- train(\n",
    " MCP.1 ~ ., \n",
    "  data = df, \n",
    "  method = \"pcr\",\n",
    "  trControl = trainControl(method = \"cv\", number = 10),\n",
    "  metric = \"RMSE\",\n",
    "  preProcess = c(\"zv\", \"center\", \"scale\"),\n",
    "  tuneLength = 20\n",
    ")\n",
    "\n",
    "# partial least squares regression\n",
    "set.seed(123)\n",
    "cv_model3 <- train(\n",
    "  MCP.1 ~ ., \n",
    "  data =df, \n",
    "  method = \"pls\",\n",
    "  trControl = trainControl(method = \"cv\", number = 10),\n",
    "  metric = \"RMSE\",\n",
    "  preProcess = c(\"zv\", \"center\", \"scale\"),\n",
    "  tuneLength = 20\n",
    ")\n",
    "\n",
    "# regularized regression\n",
    "set.seed(123)\n",
    "cv_model4 <- train(\n",
    "  MCP.1 ~ ., \n",
    "  data =df,\n",
    "  method = \"glmnet\",\n",
    "  trControl = trainControl(method = \"cv\", number = 10),\n",
    "  metric = \"RMSE\",\n",
    "  preProcess = c(\"zv\", \"center\", \"scale\"),\n",
    "  tuneLength = 10\n",
    ")\n",
    "\n",
    "library( kableExtra)\n",
    "\n",
    "# extract out of sample performance measures\n",
    "summary(resamples(list(\n",
    "  Multiple_regression = cv_model1, \n",
    "  PCR = cv_model2, \n",
    "  PLS = cv_model3,\n",
    "  Elastic_net = cv_model4,\n",
    "  MARS = tuned_mars\n",
    ")))$statistics$RMSE %>%\n",
    "  kableExtra::kable() %>%\n",
    "  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n",
    "\n",
    "#Feature interpretation\n",
    "#MARS models via earth::earth() include a backwards elimination feature selection routine \n",
    "#that looks at reductions in the GCV estimate of error as each predictor is added to the model.\n",
    "#This total reduction is used as the variable importance measure (value = \"gcv\"). \n",
    "#Since MARS will automatically include and exclude terms during the pruning process,\n",
    "#it essentially performs automated feature selection. \n",
    "#If a predictor was never used in any of the MARS basis functions in the final model (after pruning),\n",
    "#it has an importance value of zero.\n",
    "\n",
    "#while the rest of the features have an importance value of zero since they were not included in the final model.\n",
    "#Alternatively, you can also monitor the change in the residual sums of squares (RSS) as terms are added (value = \"rss\");\n",
    "#however, you will see very little difference between these methods.\n",
    "\n",
    "# variable importance plots\n",
    "p1 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = \"gcv\") + ggtitle(\"GCV\")\n",
    "p2 <- vip(tuned_mars, num_features = 40, bar = FALSE, value = \"rss\") + ggtitle(\"RSS\")\n",
    "\n",
    "gridExtra::grid.arrange(p1, p2, ncol = 2)\n",
    "\n",
    "#Its important to realize that variable importance will only measure the impact of the prediction error \n",
    "#as features are included; however, it does not measure the impact for particular hinge functions created for a given feature.\n",
    "#For example, in Figure  we see that Resistin and Glucose are the two most influential variables;\n",
    "#however, variable importance does not tell us how our model is treating the non-linear patterns for each feature.\n",
    "#Also, if we look at the interaction terms our model retained, \n",
    "#we see interactions between different hinge functions for  Resistin and Glucose.\n",
    "\n",
    "\n",
    "#MARS provides a great stepping stone into nonlinear modeling and tends to be fairly intuitive due to being closely related to multiple regression techniques. \n",
    "#They are also easy to train and tune. \n",
    "#From this illustrated how incorporating non-linear relationships via MARS modeling greatly \n",
    "#improved predictive accuracy on our Brest Cancer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages('FSelector')#Entropy and Information Gini\n",
    "install.packages('rpart')#Decesion tree  \n",
    "install.packages('caret',dependencies = 'TRUE')\n",
    "install.packages('raprt.plot')\n",
    "install.packages('data.tree')\n",
    "install.packages('dplyr')\n",
    "\n",
    "#import libraries\n",
    "library(FSelector)\n",
    "library(caret)\n",
    "library(rpart)\n",
    "library(raprt.plot)\n",
    "library(dplyr)\n",
    "library(caTools)\n",
    "library(data.tree)\n",
    "\n",
    "df=read.csv(\"C:\\\\Users\\\\VENKY\\\\Downloads\\\\dataR2.csv\")\n",
    "nrow(df)\n",
    "ncol(df)\n",
    "head(df)\n",
    "tail(df)\n",
    "str(df)\n",
    "#Conversion\n",
    "df$Classification=factor(df$Classification)\n",
    "df$Age=as.Date(df$Age,\"%y/%m/%d\")\n",
    "str(df)\n",
    "summary(df)\n",
    "#finding null values\n",
    "which(is.na(df))\n",
    "any(is.na(df))\n",
    "df[1,4]\n",
    "##Visualization\n",
    "library(ggplot2)\n",
    "# plain gragh is seen\n",
    "ggplot(data=df, aes(x=BMI, y=MCP.1))\n",
    "#add  geometry  \n",
    "ggplot(data=df, aes(x=BMI, y=MCP.1))+ geom_point()\n",
    "#add  color\n",
    "#Lines\n",
    "a=ggplot(data=df, aes(x=BMI, y=MCP.1, colour=Classification))+geom_point()\n",
    "a\n",
    "#add  size\n",
    "ggplot(data=df, aes(x=Insulin, y=MCP.1, colour=Classification, Size=BMI))+ geom_point()\n",
    "#-----ploting with layers\n",
    "p<-ggplot(data=df, aes(x=Insulin, y=MCP.1, colour=Classification))\n",
    "# point\n",
    "a+geom_point(aes(size=BMI))\n",
    "#multiple layer\n",
    "p+geom_point()+geom_line()\n",
    "##If the insulin is in the range of 0-20 , \n",
    "#MCP-1 is ranging in between 230 -1050 then there is a chance that the person can get brest cancer \n",
    "\n",
    "#------------ mapping vs settign \n",
    "r<-ggplot(data=df, aes(x=Glucose, y=MCP.1))\n",
    "# add color\n",
    "# mapping \n",
    "r+geom_point(aes(colour=Classification))\n",
    "# setting\n",
    "# mapping \n",
    "r+geom_point(aes(size=Classification))\n",
    "#If your glucose and MCP level are high then there is a change that you may get cancer\n",
    "# setting\n",
    "r+geom_point(size=10)\n",
    "#----Denisty chart\n",
    "s+geom_density(aes(fill=Classification))\n",
    "\n",
    "#converting 1 and 2 into 0 and 1\n",
    "df$Classification=factor(df$Classification,labels=c(0,1))\n",
    "\n",
    "#Model split\n",
    "library(caTools)\n",
    "set.seed(101)\n",
    "sample=sample.split(df$Classification,SplitRatio = 0.7)\n",
    "train=subset(df,sample==TRUE)\n",
    "test=subset(df,sample==FALSE)\n",
    "\n",
    "#Model Building\n",
    "tree=rpart(Classification~.,data=train)\n",
    "tree\n",
    "\n",
    "#we can plot\n",
    "plot(tree,margin=0.1)\n",
    "\n",
    "#margin is used to adjust the plot ,for viewing labels\n",
    "text(tree,use.n = TRUE,pretty = TRUE,cex=0.8)\n",
    "\n",
    "#Predictions\n",
    "pred=predict(tree,test,type='class')\n",
    "pred\n",
    "\n",
    "#Confusion matrix for evaluting the model \n",
    "confusionMatrix(pred,test$Classification)\n",
    "#Sensitivity says actual positive  cases\n",
    "#specisity says negative values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages('randomForest')\n",
    "library(randomForest)\n",
    "\n",
    "df=read.csv(\"C:\\\\Users\\\\VENKY\\\\Downloads\\\\dataR2.csv\")\n",
    "nrow(df)\n",
    "ncol(df)\n",
    "head(df)\n",
    "tail(df)\n",
    "str(df)\n",
    "#Conversion\n",
    "df$Classification=factor(df$Classification)\n",
    "str(df)\n",
    "summary(df)\n",
    "#finding null values\n",
    "which(is.na(df))\n",
    "any(is.na(df))\n",
    "\n",
    "##Visualization\n",
    "library(ggplot2)\n",
    "# plain gragh is seen\n",
    "ggplot(data=df, aes(x=BMI, y=MCP.1))\n",
    "#add  geometry  \n",
    "ggplot(data=df, aes(x=BMI, y=MCP.1))+ geom_point()\n",
    "#add  color\n",
    "#Lines\n",
    "a=ggplot(data=df, aes(x=BMI, y=MCP.1, colour=Classification))+geom_point()\n",
    "a\n",
    "#add  size\n",
    "ggplot(data=df, aes(x=Insulin, y=MCP.1, colour=Classification, Size=BMI))+ geom_point()\n",
    "#-----ploting with layers\n",
    "p<-ggplot(data=df, aes(x=Insulin, y=MCP.1, colour=Classification))\n",
    "# point\n",
    "a+geom_point(aes(size=BMI))\n",
    "#multiple layer\n",
    "p+geom_point()+geom_line()\n",
    "##If the insulin is in the range of 0-20 , \n",
    "#MCP-1 is ranging in between 230 -1050 then there is a chance that the person can get brest cancer \n",
    "\n",
    "#------------ mapping vs settign \n",
    "r<-ggplot(data=df, aes(x=Glucose, y=MCP.1))\n",
    "# add color\n",
    "# mapping \n",
    "r+geom_point(aes(colour=Classification))\n",
    "# setting\n",
    "# mapping \n",
    "r+geom_point(aes(size=Classification))\n",
    "#If your glucose and MCP level are high then there is a change that you may get cancer\n",
    "# setting\n",
    "r+geom_point(size=10)\n",
    "#----Denisty chart\n",
    "s+geom_density(aes(fill=Classification))\n",
    "\n",
    "#converting 1 and 2 into 0 and 1\n",
    "df$Classification=factor(df$Classification,labels=c(0,1))\n",
    "\n",
    "#Model splitting\n",
    "library(caTools)\n",
    "set.seed(101)\n",
    "sample=sample.split(df$Classification,SplitRatio = 0.7)\n",
    "train=subset(df,sample==TRUE)\n",
    "test=subset(df,sample==FALSE)\n",
    "\n",
    "#before Build a model we need to check the significance l\\of each variable for that we will use tunaRF function\n",
    "\n",
    "bestmtry=tuneRF(train,train$Classification,stepFactor = 1.2,improve = 0.01,trace=T,plot=T)\n",
    "#OOB nothing but prediction error\n",
    "#so our actull consider colulmns are max 3 enough for build a model\n",
    "\n",
    "library(randomForest)#its works in regression and classification\n",
    "model=randomForest(Classification~.,data=train)\n",
    "model\n",
    "\n",
    "#No. of variables tried at each split: 3\n",
    "#Evry node will split into 3 daughter nodes\n",
    "#  OOB estimate of  error rate: 32.1% its depends upon confusionmatrix \n",
    "\n",
    "#we need find the giniinformation for tree splitting\n",
    "\n",
    "importance(model)\n",
    "\n",
    "#visualizing the gini index\n",
    "varImpPlot(model)\n",
    "\n",
    "#predicting the model\n",
    "pred=predict(model,test,type = 'class')\n",
    "pred\n",
    "\n",
    "library(caret)\n",
    "#Confusion Matrix\n",
    "\n",
    "confusionMatrix(table(pred,test$Classification))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=read.csv(\"C:\\\\Users\\\\VENKY\\\\Downloads\\\\dataR2.csv\")\n",
    "nrow(df)\n",
    "ncol(df)\n",
    "head(df)\n",
    "tail(df)\n",
    "str(df)\n",
    "#Conversion\n",
    "df$Classification=factor(df$Classification)\n",
    "str(df)\n",
    "summary(df)\n",
    "#finding null values\n",
    "which(is.na(df))\n",
    "any(is.na(df))\n",
    "\n",
    "##Visualization\n",
    "library(ggplot2)\n",
    "# plain gragh is seen\n",
    "ggplot(data=df, aes(x=BMI, y=MCP.1))\n",
    "#add  geometry  \n",
    "ggplot(data=df, aes(x=BMI, y=MCP.1))+ geom_point()\n",
    "#add  color\n",
    "#Lines\n",
    "a=ggplot(data=df, aes(x=BMI, y=MCP.1, colour=Classification))+geom_point()\n",
    "a\n",
    "#add  size\n",
    "ggplot(data=df, aes(x=Insulin, y=MCP.1, colour=Classification, Size=BMI))+ geom_point()\n",
    "#-----ploting with layers\n",
    "p<-ggplot(data=df, aes(x=Insulin, y=MCP.1, colour=Classification))\n",
    "# point\n",
    "a+geom_point(aes(size=BMI))\n",
    "#multiple layer\n",
    "p+geom_point()+geom_line()\n",
    "##If the insulin is in the range of 0-20 , \n",
    "#MCP-1 is ranging in between 230 -1050 then there is a chance that the person can get brest cancer \n",
    "\n",
    "#------------ mapping vs settign \n",
    "r<-ggplot(data=df, aes(x=Glucose, y=MCP.1))\n",
    "# add color\n",
    "# mapping \n",
    "r+geom_point(aes(colour=Classification))\n",
    "# setting\n",
    "# mapping \n",
    "r+geom_point(aes(size=Classification))\n",
    "#If your glucose and MCP level are high then there is a change that you may get cancer\n",
    "# setting\n",
    "r+geom_point(size=10)\n",
    "#----Denisty chart\n",
    "s+geom_density(aes(fill=Classification))\n",
    "\n",
    "#converting 1 and 2 into 0 and 1\n",
    "df$Classification=factor(df$Classification,labels=c(0,1))\n",
    "\n",
    "library(caTools)# logistic regression\n",
    "set.seed(101)\n",
    "sample=sample.split(df$Classification,SplitRatio = 0.7)\n",
    "train=subset(df,sample==TRUE)\n",
    "test=subset(df,sample==FALSE)\n",
    "\n",
    "model=glm(Classification~.,data=train,family='binomial')#model bulding\n",
    "summary(model)\n",
    "\n",
    "Adiponectin#we need remove this one bcz its has less significance levels\n",
    "#bcz \n",
    "\n",
    "res=predict(model,test,type='response')#predictions\n",
    "res\n",
    "\n",
    "table(ActualValue=test$Classification ,PredictedValue=res>0.5)#confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
